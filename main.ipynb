{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #f36622\">2018_dsai_hw3</h1>\n",
    "<div>\n",
    "    <strong>Student ID:</strong> P76065013\n",
    "</div><div>\n",
    "    <strong>Student Name:</strong> LEUNG Yin Chung 梁彥聰\n",
    "</div>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #f36622; text-decoration: underline\">I. Introduction</h2>\n",
    "<p>\n",
    "    This assignment is to try to use a <strong style=\"color: #f36622\">sequence to sequence learning model</strong> for training a string-based numerical operation. The input data is a <strong style=\"color: #f36622\">stringified numerical operation</strong> like <code>\"535-456\"</code>, where the target result is the string based numerical equivalent, eg. <code>\"79 \"</code>. For the convenience of basic recurrent neural network implementation, I have used string padding with spaces at the end, where the length spans the range of possible input or output scenarios.\n",
    "</p><p>\n",
    "    Personally I have tried implementation using Tensorflow based on a similar sequence to sequence model official tutorial of word prediction, but an unacceptable results were experienced. Therefore, the models I present here is mainly adjusted from the <strong style=\"color: #f36622\">sample codes</strong> of the TA, and will also focus of analysis report later on.\n",
    "</p>\n",
    "<p>The corpuses are saved in the <strong style=\"color: #f36622\">corpus</strong> folder, where different datasets were used in different tutorials. The source code with the model building and data generation is saved in <strong style=\"color: #f36622\">main.py</strong>. The visual presentation of the model and details of comparison is saved in the <strong style=\"color: #f36622\">report.pdf</strong> report document.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #f36622; text-decoration: underline\">II. Corpus Simulation</h2>\n",
    "<p>\n",
    "    The corpus data algorithm is entirely <strong style=\"color: #f36622\">self-written</strong>, where you may find the algorithm in the <code>prepareData()</code> function. The <strong style=\"color: #f36622\">basic substractor dataset</strong> is saved in the 3 files ending with <code>Corpus - 3-minus.csv</code>. The data was generated once only, while different models were tested using retrieval of the saved dataset through passing <code>useOldData</code> parameter in this data preparation function. (NOTE: any old data should be placed in the same folder with <code>main.py</code>) As requeseted in the report section, there may be <strong style=\"color: #f36622\">different scenarios</strong> that we may explore. Here I will have explored 3 to 6 digits of numerical operations; substraction, addition/substraction, and multiplication operations.\n",
    "</p>\n",
    "<p>\n",
    "    Firstly, I need to identify the <strong style=\"color: #f36622\">numerical operation</strong>. Given the operation(s), the programme will <strong style=\"color: #f36622\">randomly</strong> select 2 numbers in numerically-descending order and apply a <strong style=\"color: #f36622\">random</strong> operation from the given list of operation(s). The query of operations on 2 numbers, as well as the numerical results, are then converted into strings and padded with a fixed length. For example, if it is a mulitiplication case of 4-digit numbers, the query string can be of length <code>4+1+4</code>(9), and the result string can be of length <code>4*2</code>(8).\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #f36622; text-decoration: underline\">III. Model Building</h2>\n",
    "<p>\n",
    "    The core model is based on the <strong style=\"color: #f36622\">sample code</strong> from TAs, and it's built based on Keras. <strong style=\"color: #f36622\">Variants</strong> were tested, like changing the LSTM layer counts. Another model was tried, but the performance is significantly worse than the original model. The table below shows the final accuracy of the training, validation and test datasets. A visualized model can be found in the <strong style=\"color: #f36622\">report</strong>.  [The raw output can be found in <code>finalResults.csv</code>]</p>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr style=\"background-color: #f36622; color: #fff; font-weight: 800\" ><td>Trial ID</td><td>Final Training Loss</td><td>Final Training Accuracy</td><td>Final Validation Accuracy</td><td>Final Test Accuracy</td></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "<tr><td>1</td><td>0.022</td><td>0.997</td><td>0.981</td><td>0.935</td></tr>\n",
    "<tr><td>2</td><td>0.122</td><td>0.971</td><td>0.914</td><td>0.730</td></tr>\n",
    "<tr><td>3</td><td>0.872</td><td>0.683</td><td>0.576</td><td>0.018</td></tr>\n",
    "<tr><td>4</td><td>0.855</td><td>0.687</td><td>0.613</td><td>0.053</td></tr>\n",
    "<tr><td>5</td><td>1.489</td><td>0.444</td><td>0.398</td><td>0.001</td></tr>\n",
    "<tr><td>6</td><td>1.688</td><td>0.360</td><td>0.319</td><td>0.000</td></tr>\n",
    "<tr><td>7</td><td>0.267</td><td>0.926</td><td>0.905</td><td>0.755</td></tr>\n",
    "<tr><td>8</td><td>1.115</td><td>0.585</td><td>0.565</td><td>0.072</td></tr>\n",
    "<tr><td>9</td><td>0.019</td><td>0.996</td><td>0.990</td><td>0.965</td></tr>\n",
    "<tr><td>10</td><td>0.078</td><td>0.972</td><td>0.964</td><td>0.910</td></tr>\n",
    "<tr><td>11</td><td>0.093</td><td>0.973</td><td>0.912</td><td>0.770</td></tr>\n",
    "<tr><td>12</td><td>0.023</td><td>0.996</td><td>0.979</td><td>0.932</td></tr>\n",
    "<tr><td>13</td><td>0.125</td><td>0.961</td><td>0.941</td><td>0.813</td></tr>\n",
    "<tr><td>14</td><td>0.796</td><td>0.709</td><td>0.582</td><td>0.019</td></tr>\n",
    "<tr><td>15</td><td>0.305</td><td>0.882</td><td>0.838</td><td>0.478</td></tr>\n",
    "<tr><td>16</td><td>0.766</td><td>0.710</td><td>0.591</td><td>0.024</td></tr>\n",
    "<tr><td>17</td><td>1.208</td><td>0.536</td><td>0.437</td><td>0.026</td></tr>\n",
    "<tr><td>18</td><td>0.001</td><td>1.000</td><td>0.987</td><td>0.955</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "<p>To deal with different scenarios, the programme was written in a parametric fashion, and the model above (Figure A) was trained and tested independently by feeding with parameters. The above model is compared the basic subtractor model (➖) with:</p>\n",
    "<ul>\n",
    "    <li><strong style=\"color: #f36622\">other numerical operators</strong> ([➕,➖],[✖️])  [Trials #1, #2, #3]; \n",
    "    </li><li><strong style=\"color: #f36622\">different digit sizes</strong> (4-digit, 5-digit, 6-digit numbers) [Trials #4, #5, #6]; \n",
    "</li><li><strong style=\"color: #f36622\">different LSTM hidden sizes</strong> (64, 32) [Trials #7, #8]; \n",
    "</li><li><strong style=\"color: #f36622\">different LSTM layer counts</strong> (2, 3) in the Multi-LSTM units [Trials #9, #10];\n",
    "</li><li><strong style=\"color: #f36622\">different training sizes</strong> (80%, 60% of the original) [Trials #11, #12];\n",
    "</li><li><strong style=\"color: #f36622\">more LSTM layers for other operators</strong> [Trials #13, #14, #15, #16];\n",
    "</li><li><strong style=\"color: #f36622\">newly-designed model</strong> [Trial #17]; and\n",
    "</li><li><strong style=\"color: #f36622\">larger epoch size</strong> (500) [Trial #18]\n",
    "</li>\n",
    "</ul>\n<table>\n",
    "    <thead>\n",
    "        <tr style=\"background-color: #f36622; color: #fff; font-weight: 800\" ><td>Trial ID</td><td>Digit Size</td><td>Numerical Operators</td><td>LSTM Layer Counts</td><td>Training Datasize</td><td>LSTM Hidden Units</td><td>Epoch Size</td><td>Model Number</td></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "<tr><td>1</td><td>3</td><td>➖</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>2</td><td>3</td><td>➕,➖</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>3</td><td>3</td><td>✖️</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>4</td><td>4</td><td>➖</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>5</td><td>5</td><td>➖</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>6</td><td>6</td><td>➖</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>7</td><td>3</td><td>➖</td><td>1</td><td>18,000</td>><td>64</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>8</td><td>3</td><td>➖</td><td>1</td><td>18,000</td>><td>32</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>9</td><td>3</td><td>➖</td><td>2</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>10</td><td>3</td><td>➖</td><td>3</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>11</td><td>3</td><td>➖</td><td>1</td><td>14,400</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>12</td><td>3</td><td>➖</td><td>1</td><td>10,800</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>13</td><td>3</td><td>➕,➖</td><td>2</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>14</td><td>3</td><td>✖️</td><td>2</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>15</td><td>3</td><td>➕,➖</td><td>3</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>16</td><td>3</td><td>✖️</td><td>3</td><td>18,000</td>><td>128</td><td>100</td><td>0</td></tr>\n",
    "<tr><td>17</td><td>3</td><td>➖</td><td>1</td><td>18,000</td>><td>128</td><td>100</td><td>1</td></tr>\n",
    "<tr><td>18</td><td>3</td><td>➕,➖</td><td>3</td><td>18,000</td>><td>128</td><td>500</td><td>0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #f36622; text-decoration: underline\">IV. Results Discussion</h2>\n",
    "<h3>1)\tKey Questions</h3>\n",
    "\n",
    "<h4>a)\tModel Building</h4>\n",
    "<p>The original model was based on Tensorflow word prediction sequence-to-sequence learning model.  The model was amended to have a direct LSTM layers on the original query on a shortened length of target sequence. Unfortunately, the results were <strong style=\"color: red\">unacceptably done</strong>. However, when switching to using the model from TAs, the results are very nice. It is interesting that the default model hyperparameters from Keras may be different from those I set on Tensorflow. </p>\n",
    "<p>Also, I have built another model that <strong style=\"color: red\">does not perform well</strong>. (Model #1, Trial #17). From these observations, I suppose that the criteria of setting up a pre-LSTM layer from input data may have a key success on the model building. The repeat action may just act as <strong style=\"color: red\">looking the sequence once</strong>. again manually, instead of auto-learning from a dense layer or consecutive LSTM layers. From my point of view, this action may just like in manual calculations, the digits are somehow added or subtracted are doing at different positions to be affecting the previous result digit.</p>\n",
    "<h4>b)\tCombining Adder & Subtractor</h4>\n",
    "<p>Models #2, #13, #15, #18 were testing with the required scenarios. The original model was pretty <strong style=\"color: #f36622\">awesome</strong> with 73% final test accuracy. It is known that with original model setup, it may be complicated to handle a completely different operations to attain the same results. Therefore, it is found that <strong style=\"color: #f36622\">a more complex model</strong> may let the model learns how to deal with different cases. The final test of 3-layer Multi-LSTM model with a larger epoch-size (500) would bring the final test accuracy up to 95%. However, the time cost to train the model takes significantly longer, yet no formal timer was recorded.</p>\n",
    "<h3>2)\tResults Analysis</h3>\n",
    "<h4>a)\tDifferent Number of Digits</h4>\n",
    "<p>According to Trials #4, #5, #6, the results <strong style=\"color: #f36622\">goes worse</strong> as the number of digits becomes larger. This is reasonable because the model needs to learn a longer sequence learning. It is suggested to use a larger training dataset, more complex model and larger epochSize. </p>\n",
    "<h4>b)\tDifferent Training Epoches</h4>\n",
    "<p>Smaller training data were tried to understand how the models can learn up to the training data limit in Trials #11, #12. It is interesting to see the smallest trial #12 (used with the originally 60% samples) have a <strong style=\"color: #f36622\">comparable performance</strong> (93%) to the original, while the 80% sample only have 77%. Due to limited time, no further investigation was taken. From my point of view, this have different results may be due to the availability of the data that those 20% may not have included complicated cases that may affect the performance.</p>\n",
    "<h4>c)\tLSTM Layer Adjustments</h4>\n",
    "<p><strong style=\"color: #f36622\">Less hidden units</strong> were tried with <strong style=\"color: #f36622\">lower performance </strong>results. It is noted that on halving the hidden size to 64 may still have acceptable performance at final test accuracy of 75%, but on halving once more to 32 will have unacceptable performance at 7% only. Stacking <strong style=\"color: #f36622\">more LSTM layers</strong> may have <strong style=\"color: #f36622\">performance boost</strong> according to Trials #9 vs 1, #13 vs 2. However, when stacking to 3 layers, the results is not so acceptable (Trials #10, #15). Because of a complex model, <strong style=\"color: #f36622\">more epochs</strong> of training would be needed. Therefore, in Trial #18 can bring the performance back to a nice value on 500 epoch sizes.</p>\n",
    "<h4>d)\tMultiplication Model</h4>\n",
    "<p>From the experiment results, the multiplication operation <strong style=\"color: #f36622\">does not perform well</strong> in Trials #3, #14, #16. It is observed that more complex models have brought insignificant improvements. Personally, I believe LSTM models can train such a result. Despite the fact that the results are sparse (some string combination may not be a result because a prime number is not a multiplication of 2 numbers), there are rules in compromising the product of 2 numbers. However, I think a larger dataset should be used with a longer epoch size. A pretraining of lower-digit multiplication may also be useful.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
